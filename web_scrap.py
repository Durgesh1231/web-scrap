# -*- coding: utf-8 -*-
"""web scrap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vF0dQawF4fbJH70mw77OOl-CEwjVplYy

Ans 1.

Web Scraping is the process of extracting data from websites by simulating human browsing behavior.
It involves downloading the web page and parsing the HTML content to extract specific information.
Why is it used? Web scraping is used to gather large amounts of data from websites for various purposes such as market analysis, research, and data collection.
Three areas where Web Scraping is used:
1. Price Monitoring: Scraping product prices from e-commerce websites for comparison or analysis.
2. Job Listings: Collecting job openings from various job portals for aggregating opportunities.
3. News Aggregation: Gathering headlines, articles, or other content from news websites to provide summarized updates.



ANS 2.


1. HTML Parsing: Using libraries like Beautiful Soup or lxml to parse the HTML content of web pages and extract data.
2. Regular Expressions: Using regex patterns to search for specific text or data in the HTML structure.
3. Selenium: Using browser automation tools like Selenium for scraping dynamic content that requires JavaScript rendering.
4. APIs: Some websites provide APIs that allow for easy data retrieval, often in structured formats like JSON or XML, which is a cleaner alternative to web scraping.

Ans 3.


Beautiful Soup is a Python library used for parsing HTML and XML documents. It creates parse trees from page source code, making it easier to extract specific data from a website.
Why it is used: It's commonly used for web scraping because it simplifies the process of navigating and searching through the HTML structure. It's efficient for extracting information like text, links, and images from static web pages.


Ans 4.

Flask is a lightweight Python web framework used to build web applications. In the context of a web scraping project, Flask can be used to create a user interface where users can input data (like URLs or search terms) for the scraper. Flask can serve as the back-end, triggering the scraping process and displaying the results through web pages.


Ans 5.

1. AWS Lambda: A serverless compute service used to run your web scraping code without managing servers. Lambda can be triggered by events, such as when a user submits a request via the Flask app.
2. Amazon S3 (Simple Storage Service): Used to store the scraped data or results in files. For instance, the scraped data can be saved as CSV or JSON files and stored in an S3 bucket.
3. Amazon EC2 (Elastic Compute Cloud): If more processing power is needed, EC2 instances can be used to run your web scraping scripts in a scalable manner.
4. Amazon RDS (Relational Database Service): If you need to store structured scraped data, you can use RDS to store it in a database like MySQL, PostgreSQL, or others.
5. AWS API Gateway: If you want to expose the scraping functionality as an API, you can use API Gateway to manage requests from users and invoke Lambda functions.
"
"""